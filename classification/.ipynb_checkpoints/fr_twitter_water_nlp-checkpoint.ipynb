{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP preprocessing of twitter\n",
    "\n",
    "Author: Fadoua Ghourabi (fadouaghourabi@gmail.com)\n",
    "\n",
    "Date: July 09, 2019\n",
    "\n",
    "This library provides functions for preprocessing tweets such as tokenization, removing stopwords and some patterns, lemmatization, etc. The main function ``clean_collection`` is a pipeline of preprocessing functions adapted to our purpose. It is exported to make corpora and to convert preprocessed tweet into vector representation.\n",
    "\n",
    "<font color=\"red\">The functions use libraries for processing text in french. We observe various issues that we wish to address gradually to improve the</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file contains manually labeled tweets\n",
    "tw_path = \"../datasets/twData_clean_labeled.csv\"\n",
    "tw_data = pd.read_csv(tw_path, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw_data.TwContent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_desc(tw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalization pipeline\n",
    "### 1.1 Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    '''\n",
    "    - Description:\n",
    "    lower_case converts a text to lower case\n",
    "    - History:\n",
    "    June 9, 2019 --> implementation\n",
    "    '''\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    return text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_collection(array):\n",
    "    '''\n",
    "    - Description:\n",
    "    lower_case_collection converts texts in a list to lower case\n",
    "    June 9, 2019 --> implementation\n",
    "    '''\n",
    "    array_lower = [lower_case(text) for text in array]\n",
    "    \n",
    "    return array_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower_case_collection(tw_data.TwContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    '''\n",
    "    - Description:\n",
    "    remove_punctuation removes punctuation from a text/tweet. \n",
    "    Note: \"://\", \"/\" and \".\" in urls will be removed. \n",
    "    To not loose the url information, must remove punctuation after extracting urls.\n",
    "    - History:    \n",
    "    June 9, 2019 --> implementation (credit to Shivangi)\n",
    "    to fix: \n",
    "    string.punctuation: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', apostrophe should be removed?\n",
    "    '''\n",
    "    # str.maketrans construct a mapping table where \n",
    "    # the 1st parameter is replaced by the 2nd parameter \n",
    "    # and third parameter is removed  \n",
    "    obj = str.maketrans('', '',string.punctuation)\n",
    "    \n",
    "    # text.translate applies the mapping table\n",
    "    text_unpunct = text.translate(obj)\n",
    "    \n",
    "    return text_unpunct # text with no punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_collection(array):\n",
    "    array_unpunct = [remove_punctuation(text) for text in array]\n",
    "    \n",
    "    return array_unpunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_punctuation_collection(lower_case_collection(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(text):\n",
    "    '''\n",
    "    - Description:\n",
    "    tokenize_word performs the tokenization of a tweet.\n",
    "    - Output:\n",
    "    words: a list of words in a tweet\n",
    "    - History:\n",
    "    June 9, 2019 --> implementation\n",
    "    to fix: tokenization of \"l'eau\"\n",
    "    '''\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word_collection(array):\n",
    "    array_wtokens = [tokenize_word(text) for text in array]\n",
    "    \n",
    "    return array_wtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize_word_collection(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_sent(text):\n",
    "    '''\n",
    "    - Description:\n",
    "    tokenize_sent performs the sentence tokenization of a tweet. \n",
    "    A tweet can be 1 or more sentenses delimited by \".\".\n",
    "    - Output:\n",
    "    sentences: a list of sentences in a tweet\n",
    "    - History:\n",
    "    June 9, 2019 --> implementation, probably not useful.\n",
    "    '''\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent_collection(array):\n",
    "    array_stokens = [tokenized_sent(text) for text in array]\n",
    "    \n",
    "    return array_stokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize_sent_collection(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 appos\n",
    "No appos in French?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download(stopwords) # uncomment to download required corpora, e.g. stopwords, punkt\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of french stopwords is downloaded from nltk corpus\n",
    "# the stopwords are in lower case, thus must call lower_case before removing stopwords\n",
    "# Careful: the list is not complete, e.g. \"les\" is not included.\n",
    "stop_words = set(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words):\n",
    "    '''\n",
    "    - Description:\n",
    "    remove_stopwords removes stopwords from a tweet. \n",
    "    - History:\n",
    "    June 9, 2019 --> implementation, to fix: stop_words list is not complete\n",
    "    '''\n",
    "    filtered_sentence = [] \n",
    "    # the tweet is tokenized before searching for stopwords and removing them\n",
    "    word_tokens = word_tokenize(text)                                                \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
    "    # words (-stopwords) are joined in a sentence \n",
    "    text_nostopwords =' '.join(filtered_sentence) \n",
    "    \n",
    "    return text_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_collection(array, stop_words):\n",
    "    array_nostopwords = [remove_stopwords(text,stop_words) for text in array]\n",
    "\n",
    "    return array_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_stopwords_collection(tweets, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Remove objects (urls, undesired symbols, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions\n",
    "### pattern for \\n ?\n",
    "# patterns for twitter elements \"#\", \"rt\" and \"@\"?\n",
    "# regular expression of urls\n",
    "url_pattern = r'(https?:\\/\\/)(\\s)?(www\\.)?(\\s?)(\\w+\\.)*([\\w\\-\\s]+\\/)*([\\w-]+)\\/?'\n",
    "# space symbols (e.g. ISO coding) that should be replaced by \" \"\n",
    "space_pattern = u'\\xa0'\n",
    "# alphanumeric symbols should be kept\n",
    "#nonalphanumeric_pattern = r'[^a-zA-z0-9\\s]'\n",
    "nonalphanumeric_pattern = r'[^a-zA-z0-9àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text, pattern, new=''):\n",
    "    '''\n",
    "    - Description:\n",
    "    remove_pattern either remove or replace a pattern in a text.\n",
    "    - Input:\n",
    "    text\n",
    "    pattern: pattern to be replaced\n",
    "    new: '' if not specified, otherwise some text \n",
    "    - History:\n",
    "    June 9, 2019 --> implementation\n",
    "    July 17, 2019 --> replacing with a string (new) is allowed\n",
    "    '''\n",
    "    # re.sub searches for pattern in text and replaced by the string new\n",
    "    # if new is empty, it simply removes pattern from text\n",
    "    text_clean = re.sub(pattern, new, text)\n",
    "    \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern_collection(array, url_pattern, new=''):\n",
    "    array_clean = [remove_pattern(text, url_pattern, new) for text in array]\n",
    "\n",
    "    return array_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_pattern_collection(tweets, url_pattern)\n",
    "#remove_pattern_collection(tweets, space_pattern, \" \")\n",
    "#remove_pattern_collection(tweets, nonalphanumeric_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy # uncomment to install spacy (library for lemmatization, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer # not used because not adapted to french text\n",
    "# spaCy is a free open-source library for Natural Language Processing in Python. \n",
    "# It features lemmatization, NER, POS tagging, dependency parsing, word vectors...\n",
    "import spacy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('fr') # lemmatization for french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(model, sent):\n",
    "    '''\n",
    "    - Description:\n",
    "    lemmatize computes the lemmatization of a text.\n",
    "    - Input:\n",
    "    model: basically spacy for french lemmatization\n",
    "    sent: a sentence to be lemmatized\n",
    "    - Output:\n",
    "    lemmas: pairs of (word,lemmatized instance)\n",
    "    - History:\n",
    "    June 9, 2019 --> implementation\n",
    "    July 17, 2019 --> model as parameter, removed the mapping word to lemmatized instance\n",
    "    '''\n",
    "    sentence = model(sent)\n",
    "    lemmas = []\n",
    "    for word in sentence:  \n",
    "        lemmas.append((word.text, word.lemma_))\n",
    "    \n",
    "    #new_sentence = map(lambda x:sentence.replace(x[0],x[1]),sentence)\n",
    "    return lemmas#, new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_collection(model, array):\n",
    "    '''\n",
    "    - Description:\n",
    "    lemmatize_collection computes the lemmatization of a list of texts.\n",
    "    - Input:\n",
    "    model: basically spacy for french lemmatization\n",
    "    array: list of texts to be lemmatized\n",
    "    - Output:\n",
    "    array_lemmas: pairs of (word,lemmatized instance) for each text\n",
    "    - History:\n",
    "    June 9, 2019 --> implementation\n",
    "    July 17, 2019 --> model as parameter, removed the mapping word to lemmatized instance\n",
    "    '''\n",
    "    array_stokens = tokenize_sent_collection(array)\n",
    "    array_lemmas = []\n",
    "    for para in array_stokens:\n",
    "        for sent in para:\n",
    "            array_lemmas.append(lemmatize(model, sent))\n",
    "    \n",
    "    return array_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('La', 'le'), ('vie', 'vie'), ('est', 'être'), ('courte', 'court')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(sp,'La vie est courte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: lemmatization of a collection of tweet is slow ~ 8s\n",
    "#start = time.time()\n",
    "#lemmatize_collection(sp,tweets)\n",
    "#end = time.time()\n",
    "#end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantics\n",
    "\n",
    "Library for French language? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Readability features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textstat # uncomment to install textstat for evaluating the readability. French?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_url_tweets = remove_pattern_collection(tweets, url_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join(no_url_tweets)\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.16"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.flesch_reading_ease(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.smog_index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.flesch_kincaid_grade(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.12"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.coleman_liau_index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.automated_readability_index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.53"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.dale_chall_readability_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1386"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.difficult_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.linsear_write_formula(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.04"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.gunning_fog(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16th and 17th grade'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.text_standard(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc, lem=False):\n",
    "    '''\n",
    "    '''\n",
    "    no_url = remove_pattern(doc,url_pattern)\n",
    "    doc_with_space = remove_pattern(no_url, space_pattern, \" \")\n",
    "    #print(doc_with_space)\n",
    "    lower_doc = lower_case(doc_with_space)\n",
    "    no_punc_doc = remove_punctuation(lower_doc)\n",
    "    # token_doc = tokenize_word(lower_doc) tokinezation has been already covered in remove_stopwords function\n",
    "    # print(token_doc)\n",
    "    \n",
    "    no_stop_doc = remove_stopwords(no_punc_doc,stop_words)\n",
    "    #print(no_stop_doc)\n",
    "    #no_url = remove_pattern(no_stop_doc,url_pattern)\n",
    "    #print(no_stop_doc)\n",
    "    clean = remove_pattern(no_stop_doc,nonalphanumeric_pattern) # letters with accent should be added to nonalphanumeric_pattern, otherwsie removed!\n",
    "    #clean = no_stop_doc\n",
    "    \n",
    "    if lem:\n",
    "        #print(lemmatize(sp,clean))\n",
    "        normalized = [w[1] for w in lemmatize(sp,clean)]\n",
    "        #print(doc,clean,normalized)\n",
    "        return \" \".join(normalized)\n",
    "    else:\n",
    "        return clean    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_collection(docs, lem=False):\n",
    "    clean_tweets = [clean(tw, lem) for tw in docs]\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#RCA : L'eau potable est devenue très inaccessible à Bangui https://t.co/6bJB5z0lPl\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"#RCA : L'eau potable est devenue très inaccessible à Bangui https://t.co/6bJB5z0lPl\"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rca leau potable devenir très inaccessible bangui'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(doc, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = clean_collection(tweets,lem=True)\n",
    "#tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim # uncomment to install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim #https://pypi.org/project/gensim/\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_doc = [x.split() for x in tweets_clean]\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(tok_doc)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tok_doc]\n",
    "\n",
    "# generate LDA model #### LDA model?? how it works?\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics=2, \n",
    "                                           id2word = dictionary, \n",
    "                                           passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ldamodel.print_topics(num_topics=10, num_words=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "[generate_ngrams(tw, 2) for tw in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Edit similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LD(s,t):\n",
    "    s = ' ' + s      #-------------------------------------STEP:1\n",
    "    t = ' ' + t      #--------------------------------------STEP:2\n",
    "    d = {}\n",
    "    S = len(s)\n",
    "    T = len(t)\n",
    "    for i in range(S):\n",
    "        d[i, 0] = i #---------------------------------------STEP:3\n",
    "    for j in range (T):\n",
    "        d[0, j] = j #---------------------------------------STEP:4\n",
    "    for j in range(1,T):\n",
    "        for i in range(1,S):\n",
    "            if s[i] == t[j]:\n",
    "                d[i, j] = d[i-1, j-1]\n",
    "            else:\n",
    "                d[i, j] = min(d[i-1, j], d[i, j-1], d[i-1, j-1]) + 1\n",
    "    return d[S-1, T-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1=\"potable\"\n",
    "string2=\"perturbation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LD(string1, string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x:LD(\"potable\",x),tweets[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tweets(tw1, tw2, cl=False, lem=False):\n",
    "    \n",
    "    if clean:\n",
    "        tw1 = clean(tw1, lem)\n",
    "        tw2 = clean(tw2, lem)\n",
    "    \n",
    "     \n",
    "    LD_matrix = []\n",
    "    for w in tw1.split():\n",
    "        LD_matrix_row = []\n",
    "        for w2 in tw2.split():\n",
    "            LD_matrix_row.append(LD(w, w2))\n",
    "\n",
    "        LD_matrix.append(LD_matrix_row)\n",
    "    \n",
    "    data = pd.DataFrame(LD_matrix, columns=tw2.split(), index=tw1.split())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = compare_tweets(tweets[0],tweets[1], cl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a metric for similarity between 2 sentences\n",
    "# Paper: SHORT TEXT SIMILARITY ALGORITHM BASED ON THE EDIT DISTANCE AND THESAURUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter #A counter is a container that stores elements as dictionary keys, \n",
    "                                 # and their counts are stored as dictionary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector(clean(tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector(clean(tweets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if denominator:\n",
    "        return float(numerator) / denominator\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw1_vec = text_to_vector(clean(tweets[0]))\n",
    "#print(\"tweet 1: \",clean(tweets[0]))\n",
    "tw2_vec = text_to_vector(clean(tweets[1]))\n",
    "#print(\"tweet 2: \",clean(tweets[1]))\n",
    "get_cosine(tw1_vec,tw2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw1_vec = text_to_vector(clean(tweets[0]))\n",
    "#print(\"tweet 1: \",clean(tweets[0]))\n",
    "tw2_vec = text_to_vector(clean(tweets[10]))\n",
    "#print(\"tweet 2: \",clean(tweets[10]))\n",
    "get_cosine(tw1_vec,tw2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
