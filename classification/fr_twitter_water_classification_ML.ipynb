{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of twitter water events\n",
    "### - using classical Machine Learning algorithms\n",
    "\n",
    "Author: Fadoua Ghourabi (fadouaghourabi@gmail.com)\n",
    "\n",
    "Date: version @ July 17, 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Datasets for water-related tweets ===\n",
      "Language: French\n",
      "Collection date: February 10, 2019 ~ July 20, 2019\n",
      "Location: Sfax (center), 400km (radius)\n",
      "Data size: (748, 10)\n",
      "Features: \n",
      "\n",
      "              - Timestamp: date and time of collection. \n",
      "\n",
      "              - TwDate: date and time of tweet publication. \n",
      "\n",
      "              - TwLoc: localisation of user \n",
      "\n",
      "              - TwUserName: user name\n",
      "\n",
      "              - TwUserID: user's unique ID\n",
      "\n",
      "              - TwContent: tweet message\n",
      "\n",
      "              - ContentLoc: list of locations that are included in the tweet\n",
      "\n",
      "              - urls: list of urls that are included in the tweet\n",
      "\n",
      "              - Event: label --> water shortage (1) or not (0)\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipynb.fs.full.fr_twitter_water_datasets import tweet_avg_w2v, tweet_avg_w2v_tfidf, tweet_d2v, tweet_avg_ft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score # metric to evaluate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "SVM = SVC(random_state=10)\n",
    "LR = LogisticRegression(random_state=10)\n",
    "GNB = GaussianNB()\n",
    "RF = RandomForestClassifier(random_state=10)\n",
    "GB = GradientBoostingClassifier(random_state=10)\n",
    "model_dic = {\"KNN\":KNN, \"SVM\": SVM, \"LR\": LR, \"GNB\": GNB, \"RF\": RF, \"GB\": GB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_datasets(vecs,labels,stratify=True,random=10):\n",
    "    '''\n",
    "    - Description:\n",
    "    train_test_datasets generates train and test dataset for ML algorithms.\n",
    "    - Input:\n",
    "    vecs: word vector representation of tweets\n",
    "    labels: labels of tweets (1: water shortage, 0: not water shortage)\n",
    "    - Output:\n",
    "    6 dataframes: X (vectors), y (labels), X_train, y_train, X_test, y_test\n",
    "    - History:\n",
    "    July 17, 2019 --> implementation, to fix: any datatype of vecs and labels? (check function train_test_split)\n",
    "    '''\n",
    "    X, y = vecs, labels\n",
    "    if stratify:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=random)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random)\n",
    "    \n",
    "    return X, y, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_models(dic, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    - Description:\n",
    "    ML_models applies 6 classification algorithms, namely K-nearest neighbors, support vector machines, \n",
    "    logistic regression, gradient boosting trees, random forest and Gaussien Naive Bayes. \n",
    "    It also computes, for each model, the accuracy and the confusion matrix for further evluation.\n",
    "    - Input:\n",
    "    dic: dictionary of ML algos, e.g. model_dic\n",
    "    train and test datasets\n",
    "    - Output:\n",
    "    models is a dictionary where keys are ML algos and values are tuples of \n",
    "    train and test accuracy and confusion matrix\n",
    "    - History:\n",
    "    July 17, 2019 --> implementation, to do: default hyperparameters are user, further experiments are needed\n",
    "    '''\n",
    "    models={}\n",
    "    \n",
    "    for name, algo in dic.items():\n",
    "        algo.fit(X_train, y_train)\n",
    "        train_accuracy = algo.score(X_train, y_train)\n",
    "        test_accuracy = algo.score(X_test, y_test)\n",
    "        algo_pred = algo.predict(X_test)\n",
    "        algo_conf = confusion_matrix(y_test, algo_pred)\n",
    "        models[name]=(algo,train_accuracy,test_accuracy,algo_conf)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Averaging word2vectors\n",
    "\n",
    "The vector representation of a tweet is computed as follows: \n",
    "Let $\\mathcal{V}(M)$ be the vocabulary of a model $M$ and $t_i = (w_{i1}, \\cdots, w_{in})$ be a tweet of length $n > 0$.\n",
    "- we extract words $\\mathcal{V}(M)\\cap t_i$ that are in the vocabulary of the w2v model\n",
    "- we convert the words to their w2v representation $M(w_{ij})$\n",
    "- we deduce the tweet representation  $M(t_i) = \\frac{(\\sum_{w_{ij}\\in\\mathcal{V}(M)}(M(w_{ij})))}{|\\mathcal{V}(M)\\cap t_i|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, labels = pd.DataFrame(tweet_avg_w2v[\"TwVec\"].values.tolist()), tweet_avg_w2v[\"Event\"]\n",
    "X, y, X_train, X_test, y_train, y_test = train_test_datasets(vecs,labels,stratify=True,random=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "results = ML_models(model_dic, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                       weights='uniform'),\n",
       "  0.8716577540106952,\n",
       "  0.8074866310160428,\n",
       "  array([[131,  14],\n",
       "         [ 22,  20]])),\n",
       " 'SVM': (SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "      kernel='rbf', max_iter=-1, probability=False, random_state=10,\n",
       "      shrinking=True, tol=0.001, verbose=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'LR': (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=10, solver='warn', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'GNB': (GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  0.5454545454545454,\n",
       "  0.5668449197860963,\n",
       "  array([[70, 75],\n",
       "         [ 6, 36]])),\n",
       " 'RF': (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                         n_jobs=None, oob_score=False, random_state=10, verbose=0,\n",
       "                         warm_start=False),\n",
       "  0.982174688057041,\n",
       "  0.8663101604278075,\n",
       "  array([[142,   3],\n",
       "         [ 22,  20]])),\n",
       " 'GB': (GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                             learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                             max_features=None, max_leaf_nodes=None,\n",
       "                             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                             min_samples_leaf=1, min_samples_split=2,\n",
       "                             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                             n_iter_no_change=None, presort='auto',\n",
       "                             random_state=10, subsample=1.0, tol=0.0001,\n",
       "                             validation_fraction=0.1, verbose=0,\n",
       "                             warm_start=False),\n",
       "  0.9946524064171123,\n",
       "  0.8556149732620321,\n",
       "  array([[137,   8],\n",
       "         [ 19,  23]]))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Averaging word2vectors with TF-IDF\n",
    "\n",
    "The vector representation of a tweet is computed as follows: \n",
    "Let $\\mathcal{V}(M)$ be the vocabulary of a model $M$, $t_i = (w_{i1}, \\cdots, w_{in})$ be a tweet of length $n > 0$ and TFIDF be a TF-IDF function.\n",
    "- we extract words $\\mathcal{V}(M)\\cap t_i$ that are in the vocabulary of the w2v model\n",
    "- we convert the words to their w2v representation $M(w_{ij})\\times \\text{TFIDF}(w_{ij})$\n",
    "- we deduce the tweet representation  $M(t_i) = \\frac{(\\sum_{w_{ij}\\in\\mathcal{V}(M)}(M(w_{ij}\\times \\text{TFIDF}(w_{ij}))))}{|\\mathcal{V}(M)\\cap t_i|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, labels = pd.DataFrame(tweet_avg_w2v_tfidf[\"TwVec\"].values.tolist()), tweet_avg_w2v_tfidf[\"Event\"]\n",
    "X, y, X_train, X_test, y_train, y_test = train_test_datasets(vecs,labels,stratify=True,random=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "results = ML_models(model_dic, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                       weights='uniform'),\n",
       "  0.8948306595365418,\n",
       "  0.8181818181818182,\n",
       "  array([[132,  13],\n",
       "         [ 21,  21]])),\n",
       " 'SVM': (SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "      kernel='rbf', max_iter=-1, probability=False, random_state=10,\n",
       "      shrinking=True, tol=0.001, verbose=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'LR': (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=10, solver='warn', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'GNB': (GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  0.6114081996434938,\n",
       "  0.6417112299465241,\n",
       "  array([[84, 61],\n",
       "         [ 6, 36]])),\n",
       " 'RF': (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                         n_jobs=None, oob_score=False, random_state=10, verbose=0,\n",
       "                         warm_start=False),\n",
       "  0.9875222816399287,\n",
       "  0.8770053475935828,\n",
       "  array([[143,   2],\n",
       "         [ 21,  21]])),\n",
       " 'GB': (GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                             learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                             max_features=None, max_leaf_nodes=None,\n",
       "                             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                             min_samples_leaf=1, min_samples_split=2,\n",
       "                             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                             n_iter_no_change=None, presort='auto',\n",
       "                             random_state=10, subsample=1.0, tol=0.0001,\n",
       "                             validation_fraction=0.1, verbose=0,\n",
       "                             warm_start=False),\n",
       "  0.9946524064171123,\n",
       "  0.8823529411764706,\n",
       "  array([[139,   6],\n",
       "         [ 16,  26]]))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, labels = pd.DataFrame(tweet_d2v[\"TwVec\"].values.tolist()), tweet_d2v[\"Event\"]\n",
    "X, y, X_train, X_test, y_train, y_test = train_test_datasets(vecs,labels,stratify=True,random=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "results = ML_models(model_dic, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                       weights='uniform'),\n",
       "  0.8057040998217468,\n",
       "  0.7540106951871658,\n",
       "  array([[138,   7],\n",
       "         [ 39,   3]])),\n",
       " 'SVM': (SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "      kernel='rbf', max_iter=-1, probability=False, random_state=10,\n",
       "      shrinking=True, tol=0.001, verbose=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'LR': (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=10, solver='warn', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'GNB': (GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  0.48663101604278075,\n",
       "  0.44919786096256686,\n",
       "  array([[57, 88],\n",
       "         [15, 27]])),\n",
       " 'RF': (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                         n_jobs=None, oob_score=False, random_state=10, verbose=0,\n",
       "                         warm_start=False),\n",
       "  0.9714795008912656,\n",
       "  0.7593582887700535,\n",
       "  array([[140,   5],\n",
       "         [ 40,   2]])),\n",
       " 'GB': (GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                             learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                             max_features=None, max_leaf_nodes=None,\n",
       "                             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                             min_samples_leaf=1, min_samples_split=2,\n",
       "                             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                             n_iter_no_change=None, presort='auto',\n",
       "                             random_state=10, subsample=1.0, tol=0.0001,\n",
       "                             validation_fraction=0.1, verbose=0,\n",
       "                             warm_start=False),\n",
       "  0.9946524064171123,\n",
       "  0.7700534759358288,\n",
       "  array([[141,   4],\n",
       "         [ 39,   3]]))}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, labels = pd.DataFrame(tweet_avg_ft[\"TwVec\"].values.tolist()), tweet_avg_ft[\"Event\"]\n",
    "X, y, X_train, X_test, y_train, y_test = train_test_datasets(vecs,labels,stratify=True,random=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "results = ML_models(model_dic, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                       weights='uniform'),\n",
       "  0.9251336898395722,\n",
       "  0.8663101604278075,\n",
       "  array([[131,  14],\n",
       "         [ 11,  31]])),\n",
       " 'SVM': (SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "      kernel='rbf', max_iter=-1, probability=False, random_state=10,\n",
       "      shrinking=True, tol=0.001, verbose=False),\n",
       "  0.7754010695187166,\n",
       "  0.7754010695187166,\n",
       "  array([[145,   0],\n",
       "         [ 42,   0]])),\n",
       " 'LR': (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=10, solver='warn', tol=0.0001, verbose=0,\n",
       "                     warm_start=False),\n",
       "  0.8413547237076648,\n",
       "  0.8502673796791443,\n",
       "  array([[144,   1],\n",
       "         [ 27,  15]])),\n",
       " 'GNB': (GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  0.7486631016042781,\n",
       "  0.786096256684492,\n",
       "  array([[111,  34],\n",
       "         [  6,  36]])),\n",
       " 'RF': (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                         n_jobs=None, oob_score=False, random_state=10, verbose=0,\n",
       "                         warm_start=False),\n",
       "  0.9875222816399287,\n",
       "  0.8983957219251337,\n",
       "  array([[142,   3],\n",
       "         [ 16,  26]])),\n",
       " 'GB': (GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                             learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                             max_features=None, max_leaf_nodes=None,\n",
       "                             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                             min_samples_leaf=1, min_samples_split=2,\n",
       "                             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                             n_iter_no_change=None, presort='auto',\n",
       "                             random_state=10, subsample=1.0, tol=0.0001,\n",
       "                             validation_fraction=0.1, verbose=0,\n",
       "                             warm_start=False),\n",
       "  0.9964349376114082,\n",
       "  0.9197860962566845,\n",
       "  array([[142,   3],\n",
       "         [ 12,  30]]))}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
