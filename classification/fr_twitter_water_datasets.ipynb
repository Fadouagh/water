{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter datasets for water events\n",
    "\n",
    "Author: Fadoua Ghourabi (fadouaghourabi@gmail.com)\n",
    "\n",
    "Date: July 13, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **data**: dataframe to store extracted tweet texts and meta data (date, location, etc.) before processing\n",
    "- **clean_tweets**: list to store tweet texts after nlp preprocessing (using function clean_collection)\n",
    "- **data_clean_tweets**: dataframe copy of datasets data where tweet text is replaced by clean_tweets\n",
    "- **data_clean_tweets_in_vocab**: a copy of data_clean_tweets where tweets that are not in the model vacabulary are dropped\n",
    "\n",
    "To export for classification:\n",
    "- **tweet_avg_w2v**: tweet data where vector representation is average vectors (gensim.models.word2vec)\n",
    "- **tweet_avg_w2v_tfidf**: tweet data where vector representation is average vectors multiplied with the tfidf metric (gensim.models.word2vec and sklearn's TfidfVectorizer)\n",
    "- **tweet_d2v**: twet data where vector representation is computed by gensim.models.doc2vec\n",
    "- **tweet_avg_ft**: tweet data where vector representation uses fasttext corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "#fasttext 0.9.1 \n",
    "import fasttext\n",
    "# clean_collection is a function that implements nlp preprocessing pipeline\n",
    "from ipynb.fs.full.fr_twitter_water_nlp import clean_collection \n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file contains manually labeled tweets\n",
    "tw_path = \"../datasets/twData_clean_labeled.csv\"\n",
    "# folder for corpus generated from collected tweets\n",
    "corpusdir = 'corpus/'\n",
    "# fast text coprpus (https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "fast = '/Users/basho/fadouaproject/SafeWater/model/cc.fr.300.bin'\n",
    "# folder for tweet datasets\n",
    "dataset_path = \"../datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date of the most recent collection and location of extracted tweet\n",
    "collection_date = \"February 10, 2019 ~ July 13, 2019\"\n",
    "geo_loc = \"Sfax (center), 400km (radius)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure and NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_water_tweets(desc=True):\n",
    "    \n",
    "    '''\n",
    "    - Description: \n",
    "    load_water_tweets read the tweets and other meta data from the file by tw_path and\n",
    "    returns a dataframe\n",
    "    - Input:\n",
    "    <desc> indicates wether to display description\n",
    "    - Output:\n",
    "    <tw_data> is a dataframe where columns are given by the header of the file tw_path\n",
    "    - History:\n",
    "    July 13, 2019 --> implementation, to fix: 11 columns should be 10 (extra \",\" in tw_path?)\n",
    "    July 17, 2019 --> fixed: extra column removed in tw_path\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    file = open(tw_path,\"r\")\n",
    "    tw_data = pd.read_csv(file, header=0, encoding = 'utf-8') # utf-8 for better representation of french text\n",
    "    \n",
    "    if desc:\n",
    "        print(\"=== Tweet datasets for water events ===\")\n",
    "        print(\"Language: French\")\n",
    "        print(\"Collection date: {}\".format(collection_date))\n",
    "        print(\"Location: {}\".format(geo_loc))\n",
    "        print(\"Data size: {}\".format(tw_data.shape))\n",
    "        print(\"\"\"Features: \\n\n",
    "              - Timestamp: date and time of collection. \\n\n",
    "              - TwDate: date and time of tweet publication. \\n\n",
    "              - TwLoc: localisation of user \\n\n",
    "              - TwUserName: user name\\n\n",
    "              - TwUserID: user's unique ID\\n\n",
    "              - TwContent: tweet message\\n\n",
    "              - ContentLoc: list of locations that are included in the tweet\\n\n",
    "              - urls: list of urls that are included in the tweet\\n\n",
    "              - Event: label --> water shortage (1) or not (0)\"\"\")\n",
    "        print(\"=======================================\")\n",
    "    file.close()\n",
    "    return tw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tweet datasets for water events ===\n",
      "Language: French\n",
      "Collection date: February 10, 2019 ~ July 13, 2019\n",
      "Location: Sfax (center), 400km (radius)\n",
      "Data size: (535, 10)\n",
      "Features: \n",
      "\n",
      "              - Timestamp: date and time of collection. \n",
      "\n",
      "              - TwDate: date and time of tweet publication. \n",
      "\n",
      "              - TwLoc: localisation of user \n",
      "\n",
      "              - TwUserName: user name\n",
      "\n",
      "              - TwUserID: user's unique ID\n",
      "\n",
      "              - TwContent: tweet message\n",
      "\n",
      "              - ContentLoc: list of locations that are included in the tweet\n",
      "\n",
      "              - urls: list of urls that are included in the tweet\n",
      "\n",
      "              - Event: label --> water shortage (1) or not (0)\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "data = load_water_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Event.isnull().values.any() # missing label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Event.isna().values.any() # nan label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Les gouvernorats de Siliana, Kasserine et Jend...\n",
       "1    Perturbations et coupures de l’approvisionneme...\n",
       "2    L'approvisionnement en eau potable reprendra, ...\n",
       "3    Perturbations et coupures dans l’approvisionne...\n",
       "4    Perturbations dans l’approvisionnement en eau ...\n",
       "5    La reprise sera progressive... https://t.co/6h...\n",
       "6    #Tunisie : Perturbations et coupures dans l’ap...\n",
       "7    Tunisie - Tozeur : La SONEDE rassure sur la qu...\n",
       "8    Nos gouvernants ont l'habitude de prendre de l...\n",
       "9    Jendouba nord : le vol des équipements de la S...\n",
       "Name: TwContent, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.TwContent.head(10) # to check the coding of french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = data.TwContent # variable tweets contain raw tweet text as collected by the api\n",
    "tweets.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessing(tws,lem=False):\n",
    "    ''' \n",
    "    - Description:\n",
    "    nlp_preprocessing simply calls clean_collection\n",
    "    clean_collection applies nlp preprocessing (lower, remove urls, remove stopwords) on all the tweets\n",
    "    - Input:\n",
    "    tws: raw tweet text\n",
    "    lem: lemmatization is optional, default False\n",
    "    - Output:\n",
    "    clean_tweets: prepreocessed tweet text\n",
    "    duration: computation time\n",
    "    - History:\n",
    "    July 13, 2019 --> implementation, clean_collection is not prompt\n",
    "    July 16, 2019 --> added assertion to make sure no tweet is lost\n",
    "    '''\n",
    "    start = time.time()\n",
    "    clean_tweets = clean_collection(tws, lem) \n",
    "    end = time.time()\n",
    "    duration = end - start # about 5s !\n",
    "    \n",
    "    assert len(clean_tweets) == tws.shape[0]\n",
    "    \n",
    "    return duration, clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 4.709585189819336)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing of tweets (see function clean_collection)\n",
    "# warning: slow computation ~ 5s\n",
    "start = time.time()\n",
    "_, clean_tweets = nlp_preprocessing(tweets, True) # \n",
    "end = time.time()\n",
    "len(clean_tweets), end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_column(data,column_index,newcolumn):\n",
    "    '''\n",
    "    - Description:\n",
    "    replace_column replaces an entire column with new values, \n",
    "    e.g. the raw tweet text is replaced by its nlp preprocessing\n",
    "    !! Attention !!\n",
    "    if column_index is not an existing column then a new column is added\n",
    "    - Input:\n",
    "    data: original data\n",
    "    column_index: a string indicating the column header\n",
    "    new_column: new values for the column\n",
    "    - Output:\n",
    "    newdata: new data with new column values\n",
    "    - History:\n",
    "    July 13, 2019 --> implementation\n",
    "    July 16, 2019 --> added assertions\n",
    "    '''\n",
    "    newdata = data.copy()\n",
    "    assert not (id(newdata) == id(data))\n",
    "    \n",
    "    m = newdata.shape[0]\n",
    "    newcolumn = list(newcolumn)\n",
    "    newdata[column_index] = pd.Series(newcolumn)\n",
    "    nm = len(newcolumn)\n",
    "        \n",
    "    assert (m == nm)\n",
    "    # column is replaced or new column is added\n",
    "    assert (newdata.shape == data.shape) or (newdata.shape[1] == data.shape[1]+1)\n",
    "    \n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_tweets = replace_column(data,\"TwContent\",clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112777912160, 112777706296)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(data_clean_tweets),id(data) # dataframes should not refer to the same object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make text corpus\n",
    "\n",
    "We make a corpus out of the collected tweets. The corpus is used to generate word2vec representation (see next section). We make $n$ text files for $n$ tweets. All are savec in ``corpusdir``. \n",
    "\n",
    "For the time being, when adding new tweets, delete folder ``corpusdir`` and run ``make_text(clean_tweets)``. <font color='red'>This feature should be improved so that the exsisting corpus can be extended with new data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(corpusdir):\n",
    "    os.mkdir(corpusdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one file - !! not used !!\n",
    "def make_data_from_tweets(tweets):\n",
    "    filename = 0\n",
    "    file = open(corpusdir+'data.txt','a')\n",
    "    for tw in tweets:\n",
    "        file.write(tw)\n",
    "        file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate files\n",
    "def make_text(data):\n",
    "    '''\n",
    "    - Description:\n",
    "    make_text create corpus text files, one file for each tweet.\n",
    "    - Input:\n",
    "    tweets preferably nlp preprocessed tweets\n",
    "    - Histroy:\n",
    "    July 13, 2019 --> implementation, to fix: extending the corpus with new tweets\n",
    "    '''\n",
    "    \n",
    "    filename = 0\n",
    "    for text in data:\n",
    "        filename+=1\n",
    "        file = open(corpusdir+str(filename)+'.txt','w')\n",
    "        file.write(text)#,encoding=\"UTF-8\")\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_text(clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet2vector representation (gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to NLTK dosucmentation on ``PlaintextCorpusReader``: \"Reader for corpora that consist of plaintext documents. Paragraphs are assumed to be split using blank lines. Sentences and words can be tokenized using the default tokenizers, or by custom tokenizers specificed as parameters to the constructor.\"\n",
    "\n",
    "However, our corpus already contains preprocessed tweets. We made this choice because of the language of the tweets requires non-common libraries. \n",
    "\n",
    "<font color='red'>For stronger credence, experiments should be performed: \n",
    "- ``PlaintextCorpusReader`` on raw corpora, \n",
    "- ``PlaintextCorpusReader`` on raw corpora + custom tokenizer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcorpus = PlaintextCorpusReader(corpusdir, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('comment le conduite   eau jelma sauvagement saboter réparer sou 24 heure vidéo photo tunisie sfax eau leaderstunisie',\n",
       " ['comment',\n",
       "  'le',\n",
       "  'conduite',\n",
       "  'eau',\n",
       "  'jelma',\n",
       "  'sauvagement',\n",
       "  'saboter',\n",
       "  'réparer',\n",
       "  'sou',\n",
       "  '24',\n",
       "  'heure',\n",
       "  'vidéo',\n",
       "  'photo',\n",
       "  'leaderstunisi'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clean_tweets[352],newcorpus.sents()[352])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim’s word2vec expects **a sequence of sentences as its input**. Each sentence is a list of words (utf8 strings). \n",
    "\n",
    "``class gensim.models.Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)``\n",
    "\n",
    "- Words that appear only once or twice in a corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so the model ignores them. These words are not included in the vocabulary ``w2v_model.wv.vocab`` (OOV words). To change the minimum occurance of a word to be included in the vocabulary, use parameter ``min_count``.\n",
    "\n",
    "- Word2Vec runs a neural network of default size 100. To change the size of NN, use parameter ``size``.\n",
    "\n",
    "- Word2Vec uses a context window to limit the number of words in each context. To change the window size, use parameter ``window``\n",
    "\n",
    "- The default algorithm is CBOW (parameter ``sg=0``). The use skipgram, set parameter ``sg=1``. \n",
    "\n",
    "<font color='red'>We should perform experiments with various values for the hyperparameters.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Word2Vec on newcorpus sentenses\n",
    "# CBOW algorithm, words occuring > 3 times make vocab, context window of size 3, NN of 50 layers \n",
    "w2v_model = gensim.models.Word2Vec(newcorpus.sents(), sg=0, min_count=3, window=5, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the vocabulary (276 when min_count = 5)\n",
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00502588, -0.00020879, -0.00313085, -0.04761149,  0.02591472,\n",
       "        0.0797812 ,  0.02400271, -0.02761331, -0.01668293, -0.00924041,\n",
       "        0.01550639,  0.02967573,  0.02331197, -0.01905984, -0.02850658,\n",
       "       -0.06293469,  0.02005616, -0.0118856 ,  0.03819882, -0.00791364,\n",
       "       -0.11097454, -0.0066397 ,  0.08152932, -0.04701308, -0.07478894,\n",
       "        0.01865409, -0.05734168,  0.00458778, -0.12077726,  0.02771127,\n",
       "       -0.09081082, -0.08316717, -0.07378556, -0.0551071 , -0.01240881,\n",
       "       -0.03893253,  0.00585383,  0.07512907,  0.01821466, -0.02814579,\n",
       "       -0.04035135,  0.00860792, -0.04398966, -0.06897693,  0.01197071,\n",
       "        0.03174187,  0.02844218, -0.00801674, -0.04205552,  0.03668069],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"eau\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00387644,  0.00468257,  0.00676616, -0.01473087,  0.01621496,\n",
       "        0.02416983,  0.01524329, -0.01899278, -0.01286543,  0.0028702 ,\n",
       "        0.01588697,  0.0147522 ,  0.01496758, -0.0048678 , -0.00419197,\n",
       "       -0.01462643,  0.00433477, -0.01591946,  0.02273679, -0.01218186,\n",
       "       -0.03896337, -0.00629161,  0.03724789, -0.0240705 , -0.03266382,\n",
       "       -0.00021227, -0.02231739,  0.00794812, -0.04975024,  0.01611223,\n",
       "       -0.02692081, -0.04251795, -0.02203562, -0.02840655, -0.01560746,\n",
       "       -0.00837522, -0.00395815,  0.03792381,  0.00723598, -0.01338847,\n",
       "       -0.00716835, -0.00604373, -0.01443875, -0.02419888,  0.00816183,\n",
       "        0.01295616,  0.00439254,  0.00036245, -0.00648204,  0.02234869],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"coupure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_two_words(model,w1, w2):\n",
    "    sim = model.wv.similarity(w1,w2)\n",
    "    #print(\"The similarity between <{}> and <{}>: \".format(w1, w2), sim)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9751394"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_two_words(w2v_model, 'eau', 'potable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94240123"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_two_words(w2v_model, 'eau', 'coupure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60116863"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_two_words(w2v_model, 'eau', 'gafsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7594028"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_two_words(w2v_model, 'eau', 'tunis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('le', 0.9853494763374329),\n",
       " ('leau', 0.9834550619125366),\n",
       " ('potable', 0.9751394391059875),\n",
       " ('avoir', 0.9732195734977722),\n",
       " ('projet', 0.9708139896392822),\n",
       " ('plus', 0.9697649478912354),\n",
       " ('tunisie', 0.9683387279510498),\n",
       " ('rt', 0.9657126069068909),\n",
       " ('jour', 0.9641616344451904),\n",
       " ('deau', 0.9614266753196716)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('eau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eau', 0.9424012303352356),\n",
       " ('faire', 0.9310715794563293),\n",
       " ('le', 0.928193986415863),\n",
       " ('plus', 0.9276940822601318),\n",
       " ('potable', 0.9273414611816406),\n",
       " ('rt', 0.9245010018348694),\n",
       " ('accès', 0.9230503439903259),\n",
       " ('leau', 0.9185031652450562),\n",
       " ('avoir', 0.9168297052383423),\n",
       " ('projet', 0.9137657284736633)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('coupure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potable', 0.9339042901992798),\n",
       " ('le', 0.9324987530708313),\n",
       " ('avoir', 0.9290918111801147),\n",
       " ('eau', 0.92876797914505),\n",
       " ('tout', 0.923543393611908),\n",
       " ('bangui', 0.9212305545806885),\n",
       " ('leau', 0.9203723669052124),\n",
       " ('projet', 0.9202545285224915),\n",
       " ('devenir', 0.9178816676139832),\n",
       " ('deau', 0.9155687689781189)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 1: Averaging word2vec\n",
    "\n",
    "How to compute the vector representation of a tweet from the vector representation of its words? One alternative is the sum the vector representation and divide by the lenght of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_vocabulary(model,tws,vocab):\n",
    "    '''\n",
    "    - Description:\n",
    "    check_in_vocabulary checks which words are not in the model's vocabulary. \n",
    "    These words cause errors as they cannot be converted to vectors.\n",
    "    eg: KeyError \"word 'satisfaction' not in vocabulary\"\n",
    "    - Input:\n",
    "    model: the trained word2vector model\n",
    "    tws: clean tweets\n",
    "    vocab: the model's vocabulary. In case of gensim's Word2Vec, model.wv.vocab gives a dictionary of vocabulary.\n",
    "    - Output:\n",
    "    not_in_vocabulary: list of (row_index,tweet) where tweet is composed of words \n",
    "    that are not in the model's vocabulary.\n",
    "    - History:\n",
    "    July 16, 2019 --> implementation, we choose to ignore the words that are not in the vocabulary (OOV).\n",
    "    '''\n",
    "    not_in_vocabulary = []\n",
    "    i = 0\n",
    "    for tw in tws:\n",
    "        n_words = 0\n",
    "        for w in tw.split():\n",
    "            if w in vocab: # careful! model.wv.vocab is not a complete list of unique words\n",
    "                n_words += 1\n",
    "        \n",
    "        if n_words == 0: # meaning all the words in tw are not in model's vocabulary\n",
    "            not_in_vocabulary.append((i,tw)) \n",
    "        i += 1\n",
    "    \n",
    "    return not_in_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_not_in_vocabulary(data,indexes):\n",
    "    '''\n",
    "    - Description:\n",
    "    drop_not_in_vocabulary drops from data the tweets that are not in the model's vocabulary\n",
    "    - Input:\n",
    "    data: complete data with clean tweet texts, i.e. the dataset data_clean_tweets\n",
    "    indexes: row indexes of tweets not in vocabulary\n",
    "    - Output:\n",
    "    data.drop(indexes) \n",
    "    '''\n",
    "    return data.drop(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv = check_in_vocabulary(w2v_model,clean_tweets,w2v_model.wv.vocab)\n",
    "nv # empty nv means all tweets are composed of words in w2v_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [i for (i, _) in nv]\n",
    "data_clean_tweets_in_vocab = drop_not_in_vocabulary(data_clean_tweets,indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean_tweets_in_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_avgvecs(model,tws,num_features):\n",
    "    '''\n",
    "    - Description:\n",
    "    tweets_to_avgvecs computes the vector representation of a tweet. \n",
    "    To that end, it sums the vector representations of words, then divide by the nbr of words.\n",
    "    Out of vocabulary words (OOV) are ignored.\n",
    "    numpy arrays are used for efficient array arithmetics\n",
    "    - Input:\n",
    "    model: the trained model\n",
    "    tws: clean tweets\n",
    "    num_feature: the size of the vector representation\n",
    "    - Output:\n",
    "    vectors: numpy array of stacked vector representations of tweets\n",
    "    not_in_vocabulary: tweets that are not in the vocabulary\n",
    "    - History:\n",
    "    July 16, 2019 --> implementation, \n",
    "    to fix: \n",
    "    a) one function for different models: e.g. w2v_avg, w2v_avg_tfidf, w2v_avg_ft\n",
    "    b) what to do with OOV?\n",
    "    '''\n",
    "    vocab = model.wv.vocab # assuming gensim's word2vec model\n",
    "    vectors = [] #np.empty((len(tweets), num_features)) # dtype='float32')\n",
    "    not_in_vocabulary = []\n",
    "    i = 0\n",
    "    for tw in tws:\n",
    "        tw_vec = np.zeros((num_features,), dtype='float32')\n",
    "        n_words = 0\n",
    "        \n",
    "        for w in tw.split():\n",
    "            if w in vocab: # careful! model.wv.vocab is not a complete list of unique words\n",
    "                n_words += 1\n",
    "                # summation of vectors of words in vocab\n",
    "                # OOV are ignored\n",
    "                tw_vec = np.add(tw_vec, model.wv[w]) \n",
    "                \n",
    "        \n",
    "        if (n_words > 0):\n",
    "            tw_vec = np.divide(tw_vec, n_words)\n",
    "            vectors.append(tw_vec.tolist()) #, axis=0)\n",
    "        else:\n",
    "            # not_in_vocabulary staures the tweets composed of words not in model.wv.vocab\n",
    "            not_in_vocabulary.append((i,tw)) \n",
    "        i += 1\n",
    "        \n",
    "        assert len(vectors[0]) == num_features\n",
    "\n",
    "    return (np.array(vectors),not_in_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = w2v_model.vector_size\n",
    "tws = data_clean_tweets_in_vocab.TwContent\n",
    "tw_vectors, nv = tweets_to_avgvecs(w2v_model,tws,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_avg_w2v = replace_column(data_clean_tweets_in_vocab,\"TwVec\",list(tw_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 11)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_avg_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 2: averaging word2vec with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_avgvecs_with_tfidf(model,vectorizer,tws,num_features):\n",
    "    '''\n",
    "    - Description:\n",
    "    tweets_to_avgvecs_with_tfidf computes the vector representation of a tweet. \n",
    "    To that end, it sums the vector representations of words multiplied by the tfidf representation,\n",
    "    then divide by the nbr of words.\n",
    "    Out of vocabulary words (OOV) are ignored.\n",
    "    numpy arrays are used for efficient array arithmetics\n",
    "    - Input:\n",
    "    model: the trained model\n",
    "    vectorizer: the tfidf model\n",
    "    tws: clean tweets\n",
    "    num_feature: the size of the vector representation\n",
    "    - Output:\n",
    "    vectors: numpy array of stacked vector representations of tweets\n",
    "    not_in_vocabulary: tweets that are not in the vocabulary\n",
    "    - History:\n",
    "    July 16, 2019 --> implementation, \n",
    "    to fix: \n",
    "    a) one function for different models: e.g. w2v_avg, w2v_avg_tfidf, w2v_avg_ft\n",
    "    b) what to do with OOV?\n",
    "    '''\n",
    "    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    vectors = [] #np.empty((len(tweets), num_features)) # dtype='float32')\n",
    "    not_in_vocabulary = []\n",
    "    i = 0\n",
    "    for tw in tws:\n",
    "        tw_vec = np.zeros((num_features,), dtype='float32')\n",
    "        n_words = 0\n",
    "        \n",
    "        for w in tw.split():\n",
    "            # careful! model.wv.vocab is not a complete list of unique words\n",
    "            if (w in model.wv.vocab) and (w in vectorizer.vocabulary_): \n",
    "                n_words += 1\n",
    "                vec = model.wv[w]*tfidf[w]\n",
    "                tw_vec = np.add(tw_vec, vec)\n",
    "                \n",
    "        \n",
    "        if (n_words > 0):\n",
    "            tw_vec = np.divide(tw_vec, n_words)\n",
    "            vectors.append(tw_vec.tolist()) #, axis=0)\n",
    "        else:\n",
    "            # not_in_vocabulary staures the tweets composed of words not in model.wv.vocab\n",
    "            not_in_vocabulary.append((i,tw)) \n",
    "        i += 1\n",
    "\n",
    "    return (np.array(vectors),not_in_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7465x2375 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7420 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(newcorpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2375,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = w2v_model.vector_size\n",
    "tws = data_clean_tweets_in_vocab.TwContent\n",
    "tw_vectors, nv = tweets_to_avgvecs_with_tfidf(w2v_model,vectorizer,tws,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 50)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 11)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_avg_w2v_tfidf = replace_column(data_clean_tweets_in_vocab,\"TwVec\",list(tw_vectors))\n",
    "tweet_avg_w2v_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_avg_w2v_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 3: doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_tweets(tws):\n",
    "    '''\n",
    "    - Description:\n",
    "    tag_tweets generates tweets along with iterable tags, \n",
    "    which are the input document format needed by Doc2Vec\n",
    "    - Input:\n",
    "    tws: a list of tweet where each tweet is tokenized (composed of a list of words)\n",
    "    - Output:\n",
    "    docs: tagged tweets\n",
    "    - History:\n",
    "    July 17, 2019 --> implementation\n",
    "    '''\n",
    "    docs = []\n",
    "    i = 0\n",
    "    for tw in tws:\n",
    "        document = TaggedDocument(tw, [\"t\"+str(i)])\n",
    "        docs.append(document)\n",
    "        i += 1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make tagged tweets\n",
    "docs = tag_tweets(newcorpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v vector size is 300, context window 3, min occurrence is 3, epochs for computation is 20\n",
    "d2v = gensim.models.Doc2Vec(vector_size=300, window=3, min_count=3, epochs=20)\n",
    "d2v.build_vocab(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# d2v model is trainted over the entire corpus (size is d2v.corpus_count)\n",
    "# field epochs is required, otherwise error\n",
    "d2v.train(docs, total_examples=d2v.corpus_count, epochs=d2v.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain vector representation of a sentence:\n",
    "# first tokenize the sentence\n",
    "# pass the list of words to function infer_vector\n",
    "#d2v.infer_vector([\"coupure\",\"eau\",\"gafsa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v saves the vector representation in d2v.docvecs\n",
    "# we transform the vectors to a list for easy manipulation\n",
    "tw_dic = dict(zip(d2v.docvecs.doctags, d2v.docvecs))\n",
    "vectors = []\n",
    "for _,vec in tw_dic.items():\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 11)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_d2v= replace_column(data_clean_tweets_in_vocab,\"TwVec\", vectors)\n",
    "tweet_d2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_avg_d2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet2vec representation (fasttext corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_vocabulary_ft(model,tws):\n",
    "    '''\n",
    "    - Description:\n",
    "    check_in_vocabulary_ft checks which words are not in the model's vocabulary. \n",
    "    These words cause errors as they cannot be converted to vectors.\n",
    "    - Input:\n",
    "    model: the trained fasttext model\n",
    "    tws: clean tweets\n",
    "    - Output:\n",
    "    not_in_vocabulary: list of (row_index,tweet) where tweet is composed of words \n",
    "    that are not in the model's vocabulary.\n",
    "    - History:\n",
    "    July 16, 2019 --> implementation, we choose to ignore the words that are not in the vocabulary (OOV).\n",
    "    to do: merge with functioncheck_in_vocabulary\n",
    "    '''\n",
    "    vocab = model.get_words()\n",
    "    not_in_vocabulary = []\n",
    "    i = 0\n",
    "    for tw in tws:\n",
    "        n_words = 0\n",
    "        for w in tw.split():\n",
    "            if w in vocab: \n",
    "                n_words += 1\n",
    "        \n",
    "        if n_words == 0:\n",
    "            not_in_vocabulary.append((i,tw)) \n",
    "        i += 1\n",
    "    \n",
    "    return not_in_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_ft_avgvecs(model,tws,num_features):\n",
    "    '''\n",
    "    - Description:\n",
    "    tweets_to_ft_avgvecs computes the vector representation of a tweet based on fasttext corporus for french. \n",
    "    To that end, it sums the vector representations of words, then divide by the nbr of words.\n",
    "    Out of vocabulary words (OOV) are ignored.\n",
    "    numpy arrays are used for efficient array arithmetics\n",
    "    - Input:\n",
    "    model: the trained model\n",
    "    tws: clean tweets\n",
    "    num_feature: the size of the vector representation\n",
    "    - Output:\n",
    "    vectors: numpy array of stacked vector representations of tweets\n",
    "    not_in_vocabulary: tweets that are not in the vocabulary\n",
    "    - History:\n",
    "    July 16, 2019 --> implementation, \n",
    "    to fix: \n",
    "    a) one function for different models: e.g. w2v_avg, w2v_avg_tfidf, w2v_avg_ft\n",
    "    b) what to do with OOV?\n",
    "    '''\n",
    "    vocab = model.get_words()\n",
    "    vectors = [] #np.empty((len(tweets), num_features)) # dtype='float32')\n",
    "    not_in_vocabulary = []\n",
    "    i = 0\n",
    "    for tw in tws:\n",
    "        tw_vec = np.zeros((num_features,), dtype='float32')\n",
    "        n_words = 0\n",
    "        \n",
    "        for w in tw.split():\n",
    "            if w in vocab: \n",
    "                n_words += 1\n",
    "                tw_vec = np.add(tw_vec, model[w])\n",
    "                \n",
    "        \n",
    "        if (n_words > 0):\n",
    "            tw_vec = np.divide(tw_vec, n_words)\n",
    "            vectors.append(tw_vec.tolist()) #, axis=0)\n",
    "        else:\n",
    "            # not_in_vocabulary staures the tweets composed of words not in model.wv.vocab\n",
    "            not_in_vocabulary.append((i,tw)) \n",
    "        i += 1\n",
    "\n",
    "    return (np.array(vectors),not_in_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.700091123580933"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we load the corpus indicated by the file fast\n",
    "# warning: slow computation ~ 30s (sometimes 19s)\n",
    "start = time.time()\n",
    "fasttext_model = fasttext.load_model(fast)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the vocabulary - obviously larger than water corpus\n",
    "len(fasttext_model.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimention of vector representation\n",
    "fasttext_model[\"eau\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.85414409637451"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets that are not in the vocabulary, i.e. composed of OOV only\n",
    "# warning: slow computation ~ 34s \n",
    "start = time.time()\n",
    "nv = check_in_vocabulary_ft(fasttext_model,clean_tweets)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [i for (i, _) in nv]\n",
    "data_clean_tweets_in_vocab = drop_not_in_vocabulary(data_clean_tweets,indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 10)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean_tweets_in_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.29478621482849"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing vector representation of tweets (using fasttext model)\n",
    "# warning: slow computation ~ 32s\n",
    "tws = data_clean_tweets_in_vocab.TwContent\n",
    "start = time.time()\n",
    "tw_vectors, nv = tweets_to_ft_avgvecs(fasttext_model,tws,300)\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 11)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_avg_ft = replace_column(data_clean_tweets_in_vocab,\"TwVec\",list(tw_vectors))\n",
    "tweet_avg_w2v_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
